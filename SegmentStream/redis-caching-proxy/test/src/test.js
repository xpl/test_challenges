import { equal } from 'assert'
import { sleep, randomStr, longRunningTest, env, proxy, connectToRedisFromTestsRunner } from './util'

//  -----------------------------------------------------------------------------------

describe ('Redis Caching Proxy End-To-End Test', () => {
    
    const redis = connectToRedisFromTestsRunner () // suspends tests until connected + disconnects when done

//  -----------------------------------------------------------------------------------

    it ('responds with 404 on unknown key', async () => {

        let error

        try       { await proxy.get ('SOME_SURELY_NONEXISTENT_KEY') }
        catch (e) { error = e }
        
        equal (error.httpStatus, 404)
    })

//  -----------------------------------------------------------------------------------

    it ('implements cached GET for keys', async () => {

        const key = 'some ?& key' + randomStr () // add URL-escaped characters to check if it's handled correctly

    /*  Test that GET works (using a key name that gets URI-encoded)    */

        await redis.set (key, 'bar')
        equal (await proxy.get (key), 'bar')

    /*  Test that caching works — change the value and make sure that our proxy returns the old value!  */

        await redis.set (key, 'qux')
        equal (await proxy.get (key), 'bar') // it's still "bar", yay!
    })

//  -----------------------------------------------------------------------------------

    it ('implements cache eviction (max keys)', longRunningTest (async () => {

    /*  The test is hard-coded (for clarity) — assuming CACHE_MAX_KEYS=3    */

        equal (env.CACHE_MAX_KEYS, "3")

    /*  Generate 4 test keys  */

        const [foo1, foo2, foo3, foo4] = [randomStr (), randomStr (), randomStr (), randomStr ()]

        await redis.set (foo1, 'foo1')
        await redis.set (foo2, 'foo2')
        await redis.set (foo3, 'foo3')
        await redis.set (foo4, 'foo4')

    /*  Populate the cache...  */

        equal (await proxy.get (foo1), 'foo1') // NB: will have become the oldest item 
        equal (await proxy.get (foo2), 'foo2')
        equal (await proxy.get (foo3), 'foo3')

    /*  Make the oldest key get evicted, by reading +1 item    */

        equal (await proxy.get (foo4), 'foo4') // evicts foo1...
 
    /*  Update values in Redis  */

        await redis.set (foo1, 'BAR1')
        await redis.set (foo2, 'BAR2')
        await redis.set (foo3, 'BAR3')
        await redis.set (foo4, 'BAR4')

    /*  Make sure that the proxy returns OLD (i.e. cached) values for the first 3 keys and the NEW value for the evicted key.  */

        equal (await proxy.get (foo4), 'foo4')
        equal (await proxy.get (foo3), 'foo3')
        equal (await proxy.get (foo2), 'foo2')
        equal (await proxy.get (foo1), 'BAR1') // ← it's not from the cache (cuz `foo1` had been evicted outta there previously)
    }))

//  -----------------------------------------------------------------------------------

    it ('implements global expiry (TTL) for keys', longRunningTest (async () => {

        equal (env.CACHE_MAX_TTL_MS, "1000") // NB: ensure that the env var has been passed correctly

        const key = randomStr ()

        await redis.set (key, 'foo')
        equal (await proxy.get (key), 'foo')

        await redis.set (key, 'BAR')
        equal (await proxy.get (key), 'foo') // ← old value

        await sleep (Number (env.CACHE_MAX_TTL_MS))
        equal (await proxy.get (key), 'BAR') // ← NEW value (because the cache entry had been expired upon CACHE_MAX_TTL_MS)
    }))

//  -----------------------------------------------------------------------------------

    it ('processes concurrent requests in parallel', longRunningTest (async () => {

        const NUM_TEST_REQUESTS = 500

    /*  Initiate 500 concurrent requests to our proxy service, reading a nonexistent key — so it's guaranteed
        that each request goes all the way back to Redis, not getting short-circuited in the LRU cache! We need this
        to make our requests persist for a while, so that the actual parallelism could take place and be measured.    */

        const parallelRequests = Array.from ({ length: NUM_TEST_REQUESTS }).map (async () => {

            const { headers } = await proxy.fetch ('SOME_SURELY_NONEXISTENT_KEY')

        /*  For each request, emit two timing events (using debug headers generated by the service) */

            return [
                { goneActive: true,  when: Number (headers.get ('X-Time-Started')) }, // when the request has started processing internally
                { goneActive: false, when: Number (headers.get ('X-Time-Ended')) }    // when it's done processing internally
            ]
        })
        
    /*  Wait until every request finishes, collect timing events (sorted from the first to the last)  */

        const timings = (await Promise.all (parallelRequests)).flat ().sort ((a, b) => a.when - b.when)

    /*  Now analyze the recorded timings, keeping track of how many simultaneously processing requests we encounter...     */

        let numSimultaneouslyActiveRequests = 0

    /*  For a sequential processing it would've looked like that (no overlaps):
        
            1. === A started ===               numSimultaneouslyActiveRequests = 1
            2. === A ended   ===               numSimultaneouslyActiveRequests = 0

            3. === B started ===               numSimultaneouslyActiveRequests = 1
            4. === B ended   ===               numSimultaneouslyActiveRequests = 0
               ...

        And for a parallel processing it would've been (at least some overlaps):

            1. === A started ===               numSimultaneouslyActiveRequests = 1
            2.     === B started ===           numSimultaneouslyActiveRequests = 2
            3. === A ended   ===               numSimultaneouslyActiveRequests = 1
            4.     === B ended   ===           numSimultaneouslyActiveRequests = 0
               ...
        */

        const simultaneouslyActiveRequestsOverTime = [...(function* () {

            for (const { goneActive } of timings) {
                
                numSimultaneouslyActiveRequests += (goneActive ? 1 : -1)
                
                if (goneActive) yield numSimultaneouslyActiveRequests
            }
        })()]

    /*  Should have at least 1 occurence of a simultaneous request processing!     */

        equal (Math.max (...simultaneouslyActiveRequestsOverTime) > 1, true)

        after (() => console.info ('\tObserved parallelism level (mean):',
                                   simultaneouslyActiveRequestsOverTime.reduce ((a, b) => a+b) / NUM_TEST_REQUESTS))

    }))

//  -----------------------------------------------------------------------------------

})
